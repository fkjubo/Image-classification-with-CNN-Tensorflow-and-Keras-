install.packages("EBImage")
library(EBImage)
install.packages("imager")
library(imager)
setwd("C:\Users\Jubo\Documents\Image problem")
setwd("\Users\Jubo\Documents\Image problem")
setwd('\Users\Jubo\Documents\Image problem')
setwd('C:\Users\Jubo\Documents\Image problem')
setwd('\Users\Jubo\Documents\Image problem')
setwd('/Users/Jubo/Documents/Image problem')
pic1 <- load.image(c(car1.jpg, car2.jpg, cat.jpg, cat2.jpg, bike1.jpg,
bike2.jpg))
pic1 <- load.image(c(car1, car2, cat1, cat2, bike1,
bike2))
?load.image
source("http://bioconductor.org/biocLite.R")
biocLite("EBImage")
library(EBImage)
pic1 <- c(car1, car2, cat1, cat2, bike1,
bike2))
pic1 <- c(car1, car2, cat1, cat2, bike1,
bike2)
pic1 <- c(car1.jpg, car2, cat1, cat2, bike1,
bike2)
pic1 <- c("car1.jpg", "car2.jpg", "cat1.jpg", "cat2.jpg", "bike1.jpg",
"bike2.jpg")
train <- for (i in 1:6) {train[i] <- readImage(pic1[i])}
train <- for (i in 1:6) {train[[i]] <- readImage(pic1[i])}
train <- list()
for (i in 1:6) {train[[i]] <- readImage(pic1[i])}
train
pic2 <- c("cat3.jpg", "car3.jpg", "bike3.jpg")
test <- list()
for (i in 1:3) {test[[i]] <- readImage(pic2[i])}
print(train[[6]])
summary(train[[6]])
display(train[[6]])
plot(train[[6]])
str(train)
for(i in 1:6) {train[[i]] <- resize(train[[i]], 100, 100)}
str(train)
for(i in 1:3) {test[[i]] <- resize(test[[i]], 100, 100)}
train <- combine(train)
test <- combine(test)
x <- tile(train. 2)
x <- tile(train, 2)
display(x)
str(train)
train <- aperm(train, (4, 1, 2, 3))
test <- aperm(test, (4, 1, 2, 3))
train <- aperm(train, c(4, 1, 2, 3))
test <- aperm(test, c(4, 1, 2, 3))
y <- tile(test, 1)
display(x)
display(y)
y <- tile(test, 3)
display(y)
y <- tile(test, 1)
display(y)
x <- tile(train, 2)
display(x)
pic1 <- c("car1.jpg", "car2.jpg", "cat1.jpg", "cat2.jpg", "bike1.jpg",
"bike2.jpg")
pic2 <- c("cat3.jpg", "car3.jpg", "bike3.jpg")
train <- list()
for (i in 1:6) {train[[i]] <- readImage(pic1[i])}
test <- list()
for (i in 1:3) {test[[i]] <- readImage(pic2[i])}
print(train[[6]])
summary(train[[6]])
display(train[[6]])
plot(train[[6]])
str(train)
# resize
for(i in 1:6) {train[[i]] <- resize(train[[i]], 100, 100)}
for(i in 1:3) {test[[i]] <- resize(test[[i]], 100, 100)}
train <- combine(train)
test <- combine(test)
x <- tile(train, 2)
display(x)
y <- tile(test, 1)
display(y)
x <- tile(train, 2)
display(x)
train <- aperm(train, c(4, 1, 2, 3))
test <- aperm(test, c(4, 1, 2, 3))
trainResponse <- c(1,1,0,0,2,2)
testResponse <- c(0,1,2)
trainLabels <- to_categorical(trainResponse)
testLabels <- to_categorical(testLabels)
library(keras)
library(EBImage)
trainLabels <- to_categorical(trainResponse)
testLabels <- to_categorical(testLabels)
trainLabels <- to_categorical(trainResponse)
testLabels <- to_categorical(testResponse)
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
# compile
model %>%
compile(loss = "categorical_croossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
# fitting data
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 200,
batch_size = 32)
?loss_categorical_crossentropy
?compile
?compile
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
# compile
model %>%
compile(loss = "categorical_crossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
# fitting data
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 200,
batch_size = 32)
model %>% evaluate(train, trainLabels)
pred <- model %>% predict_classes(train)
table(Predicted= pred, actual = trainResponse)
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
# compile
model %>%
compile(loss = "categorical_crossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
# fitting data
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 100,
batch_size = 32)
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
# compile
model %>%
compile(loss = "categorical_crossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
# fitting data
history <- model %>%
fit(train,
trainLabels,
#validation_split = .2,
epochs = 100,
batch_size = 32)
pred <- model %>% predict_classes(train)
table(Predicted= pred, actual = trainResponse)
library(keras)
library(EBImage)
library(keras)
library(EBImage)
library(keras)
library(EBImage)
pic1 <- c("car1.jpg", "car2.jpg","car4.jpg","car5.jpg","car6.jpg","car8.jpg",
"cat1.jpg", "cat2.jpg", "cat4.jpg","cat5.jpg","cat6.jpg","cat7.jpg",
"bike1.jpg","bike2.jpg","bike4.jpg")
pic2 <- c("cat3.jpg", "car3.jpg", "bike3.jpg")
train <- list()
for (i in 1:15) {train[[i]] <- readImage(pic1[i])}
test <- list()
for (i in 1:3) {test[[i]] <- readImage(pic2[i])}
str(train)
for(i in 1:6) {train[[i]] <- resize(train[[i]], 100, 100)}
for(i in 1:15) {train[[i]] <- resize(train[[i]], 100, 100)}
for(i in 1:3) {test[[i]] <- resize(test[[i]], 100, 100)}
train <- combine(train)
test <- combine(test)
x <- tile(train, 2)
display(x)
train <- aperm(train, c(4, 1, 2, 3))
test <- aperm(test, c(4, 1, 2, 3))
testResponse <- c(0,1,2)
trainResponse <- c(1,1,1,1,1,1,
0,0,0,0,0,0,
2,2,2)
trainLabels <- to_categorical(trainResponse)
testLabels <- to_categorical(testResponse)
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
# compile
model %>%
compile(loss = "categorical_crossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
# fitting data
history <- model %>%
fit(train,
trainLabels,
validation_split = .1,
epochs = 100,
batch_size = 32)
model %>% evaluate(train, trainLabels)
pred <- model %>% predict_classes(train)
table(Predicted= pred, actual = trainResponse)
prob <- model %>% predict_proba(train)
prob
pred <- model %>% predict_classes(test)
table(Predicted= pred, actual = trainResponse)
pred <- model %>% predict_classes(train)
table(Predicted= pred, actual = trainResponse)
print(train[[6]])
summary(train[[6]])
display(train[[6]])
library(keras)
library(EBImage)
pic1 <- c("car1.jpg", "car2.jpg","car4.jpg","car5.jpg","car6.jpg","car8.jpg",
"cat1.jpg", "cat2.jpg", "cat4.jpg","cat5.jpg","cat6.jpg","cat7.jpg",
"bike1.jpg","bike2.jpg","bike4.jpg")
pic2 <- c("cat3.jpg", "car3.jpg", "bike3.jpg")
train <- list()
for (i in 1:15) {train[[i]] <- readImage(pic1[i])}
test <- list()
for (i in 1:3) {test[[i]] <- readImage(pic2[i])}
print(train[[6]])
summary(train[[6]])
display(train[[6]])
plot(train[[6]])
str(train)
for(i in 1:15) {train[[i]] <- resize(train[[i]], 100, 100)}
for(i in 1:3) {test[[i]] <- resize(test[[i]], 100, 100)}
summary(train[[6]])
str(train)
train <- combine(train)
test <- combine(test)
str(train)
x <- tile(train, 2)
display(x)
y <- tile(test, 1)
display(y)
str(train)
train <- aperm(train, c(4, 1, 2, 3))
test <- aperm(test, c(4, 1, 2, 3))
trainResponse <- c(1,1,1,1,1,1,
0,0,0,0,0,0,
2,2,2)
testResponse <- c(0,1,2)
trainLabels <- to_categorical(trainResponse)
testLabels <- to_categorical(testResponse)
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu",
input_shape = c(100,100,3)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 32,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_conv_2d(filters = 64,
kernel_size = c(3,3),
activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = .25) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = .35) %>%
layer_dense(units = 3, activation = "softmax")
model %>%
compile(loss = "categorical_crossentropy",
optimizer = optimizer_sgd(lr = .002, decay = 1e-6, momentum = .9, nesterov = T),
metrics = "accuracy")
history <- model %>%
fit(train,
trainLabels,
validation_split = .1,
epochs = 100,
batch_size = 32)
model %>% evaluate(train, trainLabels)
pred <- model %>% predict_classes(train)
prob <- model %>% predict_proba(train)
table(Predicted= pred, actual = trainResponse)
plot(history)
?fit
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 300,
steps_per_epoch = 200,
batch_size = 32)
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 300,
steps_per_epoch = 200,
validation_steps = 2,
batch_size = 32)
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 300,
steps_per_epoch = 200,
validation_steps = 2,
#batch_size = 32)
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 300,
steps_per_epoch = 200,
validation_steps = 2)
history <- model %>%
fit(train,
trainLabels,
validation_split = .2,
epochs = 10,
steps_per_epoch = 100,
validation_steps = 2)
plot(history)
model %>% evaluate(train, trainLabels)
prob <- model %>% predict_proba(train)
table(Predicted= pred, actual = trainResponse)
plot(history)
history <- model %>%
fit(train,
trainLabels,
validation_split = .1,
epochs = 300,
#steps_per_epoch = 100,
#validation_steps = 2,
batch_size = 32)
plot(history)
model %>% evaluate(train, trainLabels)
pred <- model %>% predict_classes(train)
prob <- model %>% predict_proba(train)
table(Predicted= pred, actual = trainResponse)
